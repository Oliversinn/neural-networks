{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oliver Mazariegos - 16043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "## Clasificación de tipos de ropa con imagenes\n",
    "\n",
    "\n",
    "En este notebook se estará creando una red neuronal capaz de clasificar ropa a partir de una foto. Las fotos fueron obtenidas de un dataset publicado en kaggle https://www.kaggle.com/zalando-research/fashionmnist y fue descargado directo de su repositorio original en github https://github.com/zalandoresearch/fashion-mnist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist_reader\n",
    "from functools import reduce\n",
    "from scipy import optimize as op\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones utiles para el manejo de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list_of_arrays = lambda list_of_arrays: reduce (\n",
    "    lambda acc, v: np.array([*acc.flatten(), *v.flatten()],dtype=np.float64),\n",
    "    list_of_arrays\n",
    ")\n",
    "\n",
    "def flat_matrixes(lista):\n",
    "    flatten = []\n",
    "    for matris in lista:\n",
    "        # se aplanan por dentro\n",
    "        flatten = [*flatten,*(matris.flatten())]\n",
    "    # Se terminan de aplanar\n",
    "    return np.array(flatten).flatten()\n",
    "    \n",
    "\n",
    "\n",
    "def inflate_matrixes(flat_thetas, shapes):\n",
    "    layers = len(shapes) + 1\n",
    "    sizes = [shape[0] * shape[1] for shape in shapes]\n",
    "    steps = np.zeros(layers, dtype=int)\n",
    "    for i in range(layers - 1):\n",
    "        steps[i+1] = steps[i] + sizes[i]\n",
    "    return [\n",
    "        flat_thetas[steps[i]:steps[i+1]].reshape(*shapes[i])\n",
    "        for i in range(layers - 1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones anteriores son aplanadoras e infladoras de matrices. Esto debido que utilizaremos matrices de transición, peso y error para hacer calculos para las capas y neuronas dentro de la red neuronal. Pero para el algoritmo de optimización este recibe y devuelve estas matrices de forma aplanada en una lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de pertenencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion que utilizaremos para calcular la probabilidad de pertenencia sera la sigmoide. Esta funcion es utilizada para muchas curvas de aprendizaje de sistemas complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una parte fundamental del algoritmo que emplearemos para la creación de la red neuronal es el `Feed Forward`. Este es un algoritmo que calcula las matrices de activacion para cada una de las capas de la red neuronal. Siendo la primer capa las entradas de la red (la imagen) y la ultima capa el resultado que genera (la prediccion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(thetas, X):\n",
    "    # Lista de activaciones\n",
    "    a = [X] \n",
    "    for i in range(len(thetas)): \n",
    "        a.append(\n",
    "            # a\n",
    "            sigmoid(\n",
    "                # z\n",
    "                np.matmul(\n",
    "                    # agrega bias\n",
    "                    np.hstack((\n",
    "                        np.ones(len(a[i])).reshape(len(a[i]), 1),\n",
    "                        a[i]\n",
    "                    )),\n",
    "                    thetas[i].T\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `Back Propagation` es el algoritmo encargado de detectar el error y el impacto de cada capa sobre el error del resultado final. El proposito de este algoritmo es calcular las matrices Delta el cual contiene el gradiente; que es utilizado para optimizar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(flat_thetas, shapes, X, Y):\n",
    "    # 1\n",
    "    delta = []\n",
    "    m, layers = len(X), len(shapes) + 1\n",
    "    thetas = inflate_matrixes(flat_thetas, shapes)\n",
    "     # 2.2\n",
    "    a = feed_forward(thetas, X)\n",
    "    deltas = [ *range(layers -1), a[-1] - Y ] \n",
    "    # 2.4\n",
    "    for i in range(layers-2, 0, -1): # loop desde la ultima capa hasta la segunda (en reversa)\n",
    "        deltas[i] = np.matmul(deltas[i+1],(thetas[i])[:, 1:]) * (a[i] * (1 - a[i]))\n",
    "    # 2.5 y 3 \n",
    "    for i in range(layers-1): # loop de capa 0 a capa L-1\n",
    "        delta.append((np.matmul(\n",
    "            deltas[i+1].T, \n",
    "            np.hstack(( # se agrega bias a \n",
    "                        np.ones(len(a[i])).reshape(len(a[i]), 1),\n",
    "                        a[i]\n",
    "                    ))\n",
    "        )) / m)\n",
    "    return flat_matrixes(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de costo es la encargada de detectar que tan preciso o impreciso es la prediccion de nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(flat_thetas, shapes, X, Y):\n",
    "    a = feed_forward(\n",
    "        inflate_matrixes(flat_thetas, shapes),\n",
    "        X\n",
    "    )\n",
    "    return -(Y * np.log(a[-1]) + (1 - Y) * np.log(1 - a[-1])).sum() / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset esta conformado por imagenes de 28x28 resultando en 784 bits. En cada bit hay un valor entre 0 y 255, donde 0 representa al color negro y 255 al color blanco. Ademas el dataset incluye a que tipo de prenda pertenece la imagen. Existen 10 categorias de ropa representadas por un numero de la siguiente manera:\n",
    "\n",
    "* 0 T-shirt/top\n",
    "* 1 Trouser\n",
    "* 2 Pullover\n",
    "* 3 Dress\n",
    "* 4 Coat\n",
    "* 5 Sandal\n",
    "* 6 Shirt\n",
    "* 7 Sneaker\n",
    "* 8 Bag\n",
    "* 9 Ankle boot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset de entrenamiento contiene 60000 filas, cada fila representando una prenda distinta. Debido a que la derivada de la sigmoide esta acotada normalizaremos el dataset diviendolo dentro de 1000 para que la derivada no se indefina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('',kind='train')\n",
    "X = X_train / 1000\n",
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra red neuronal dara como resultado un vector con shape (10,1) mientras que la categoria brindada por el dataset tiene un shape de (1,) por lo que le haremos una conversion a este mismo para que coincidan las shapes a la hora de comparar efectividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((X.shape[0], 10))\n",
    "for i in range(m):\n",
    "    Y[i][y_train[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal tendrá 100 neuronas ocultas y como previamente discutido 10 neuronas de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_NEURONS = 100\n",
    "OUTPUT_NEURONS = 10\n",
    "\n",
    "theta_shapes = np.array([\n",
    "    [HIDDEN_NEURONS, n + 1],\n",
    "    [OUTPUT_NEURONS, HIDDEN_NEURONS + 1]\n",
    "])\n",
    "\n",
    "\n",
    "flat_thetas = flatten_list_of_arrays(\n",
    "    [np.random.rand(*theta_shape) / 1000 for theta_shape in theta_shapes]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizacion del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar la red neuronal se utilizara la funcion de optimize.minimize la cual recibe un modelo inicial aleatorio para comenzar la optimización, la función de costo, el set de entrenamiento, el resultado y la funcion que genera el gradiente que en nuestro caso es el `Back Propagation`. Ademas recibe como parametro el método de optimización a utilizar que en este caso será `L-BFGS-B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = op.minimize(\n",
    "    fun=cost_function,\n",
    "    x0=flat_thetas,\n",
    "    args=(theta_shapes,X,Y),\n",
    "    method='L-BFGS-B',\n",
    "    jac=back_propagation,\n",
    "    options={'disp':True, 'maxiter':400}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Resultante\n",
    "\n",
    "Como discutido anteriormente, la función optimizadora da los tethas optimos en forma de lista, por lo que tendremos que inflarla para obtener su forma matricial y poder utilizarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.51394812e-01,  6.49216013e-04,  2.65023664e-04, ...,\n",
       "       -8.46816999e-01, -5.61329764e-01, -3.38209573e+00])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = inflate_matrixes(result.x,theta_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "El dataset de entrenamiento tambien tiene los mismos atributos que el dataset de entrenamiento pero contiene 10000 imagene/prendas que no fueron consideradas en el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = mnist_reader.load_mnist('', kind='t10k')\n",
    "X = X_test / 1000\n",
    "m, n = X.shape\n",
    "Y = np.zeros((X.shape[0], 10))\n",
    "for i in range(m):\n",
    "    Y[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = feed_forward(modelo,X)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certeza del Modelo\n",
    "\n",
    "Para calcular la certeza del modelo se utilizara la relación de $ \\frac{Predicciones correcta}{Número total de prendas} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, real):\n",
    "    correctos = 0\n",
    "    for i in range(len(real)):\n",
    "        if np.argmax(real[i]) == np.argmax(prediction[i]):\n",
    "            correctos += 1\n",
    "    return correctos/len(real)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La certeza de la red neuronal es de:  0.8822\n"
     ]
    }
   ],
   "source": [
    "print(\"La certeza de la red neuronal es de: \",accuracy(prediccion,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Se logró obtener una red neuronal que clasificá ropa con tan solo 400 iteraciones de aprendizaje con el algoritmo de `Back Propagation con Feed Forward`, utilizando 100 neuronas ocultas con una precisión del 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
